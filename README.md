# Akademik Makalelerin AraÅŸtÄ±rma AlanlarÄ±na GÃ¶re KÃ¼melenmesi

Bu proje, ArXiv veritabanÄ±ndan toplanan akademik makaleleri iÃ§eriklerine gÃ¶re otomatik olarak kÃ¼melere ayÄ±rmak ve gÃ¶rselleÅŸtirmek iÃ§in **PySpark** kullanÄ±r.

## ğŸ¯ Proje AmacÄ±

- ArXiv'den akademik makaleleri toplamak
- Makaleleri iÃ§eriklerine gÃ¶re otomatik kÃ¼melere ayÄ±rmak  
- KÃ¼meleri gÃ¶rselleÅŸtirmek ve analiz etmek
- AraÅŸtÄ±rma alanlarÄ±nÄ± keÅŸfetmek
- BÃ¼yÃ¼k veri iÅŸleme iÃ§in PySpark'Ä±n gÃ¼cÃ¼nden yararlanmak

## ğŸ› ï¸ Teknolojiler

- **PySpark**: BÃ¼yÃ¼k veri iÅŸleme ve makine Ã¶ÄŸrenmesi
- **ArXiv API**: Akademik makale verisi toplama
- **Python**: Ana programlama dili
- **Matplotlib/Plotly/Seaborn**: GÃ¶rselleÅŸtirme
- **NLTK**: DoÄŸal dil iÅŸleme
- **Pandas**: Veri manipÃ¼lasyonu
- **WordCloud**: Kelime bulutlarÄ±

## ğŸ“ Proje YapÄ±sÄ±

```
BigData/
â”œâ”€â”€ src/                           # Kaynak kodlar
â”‚   â”œâ”€â”€ arxiv_data_collector.py   # ArXiv veri toplama
â”‚   â””â”€â”€ spark_clustering.py       # PySpark kÃ¼meleme
â”œâ”€â”€ data/                         # Veri dosyalarÄ±
â”œâ”€â”€ visualizations/              # GÃ¶rselleÅŸtirmeler  
â”œâ”€â”€ main.py                      # Ana Ã§alÄ±ÅŸtÄ±rma scripti
â”œâ”€â”€ requirements.txt             # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
â””â”€â”€ README.md                    # Bu dosya
```

## ğŸš€ Kurulum

### 1. Gereksinimleri YÃ¼kleyin

```bash
# Python paketlerini yÃ¼kle
pip install -r requirements.txt
```

4. **Gerekli dizinleri oluÅŸturun:**
```bash
mkdir -p data visualizations logs
```

### ğŸ§ª Kurulum Testi

```bash
# Virtual environment'Ä± aktifleÅŸtirin
source academic_env/bin/activate
```

## ğŸ“ˆ KullanÄ±m

### âš ï¸ Ã–nemli: Her kullanÄ±mdan Ã¶nce

```bash
# Virtual environment'Ä± aktifleÅŸtirin
source academic_env/bin/activate
```

### ğŸ“Š Tam Pipeline

TÃ¼m sÃ¼reci (veri toplama + kÃ¼meleme) Ã§alÄ±ÅŸtÄ±rmak iÃ§in:

```bash
# Tam pipeline (1000 makale ile)
python main.py --full-pipeline --max-results 1000

# Sadece veri toplama
python main.py --collect-data --max-results 500

# Sadece kÃ¼meleme (varolan veri ile)
python main.py --cluster --data-file data/arxiv_papers.csv
```

### âš™ï¸ GeliÅŸmiÅŸ Parametreler

```bash
# BÃ¼yÃ¼k veri seti ile (5000 makale)
python main.py --full-pipeline --max-results 5000 --vocab-size 10000

# Ã–zel veri dosyasÄ± ile kÃ¼meleme
python main.py --cluster --data-file my_data.csv --vocab-size 3000
```

## ğŸ“Š Ã–zellikler

### ğŸ” Veri Toplama
- ArXiv API'sinden otomatik veri toplama
- Ã‡oklu kategori desteÄŸi
- Metin temizleme ve Ã¶n iÅŸleme
- Duplicate detection

### ğŸ§  Makine Ã–ÄŸrenmesi (PySpark)
- **TF-IDF** Ã¶zellik Ã§Ä±karma
- **K-means** kÃ¼meleme algoritmasÄ±
- Otomatik optimal k bulma (Elbow method + Silhouette analysis)
- **Silhouette Score** ile model deÄŸerlendirme

### ğŸ“Š GÃ¶rselleÅŸtirme
- KÃ¼me boyutlarÄ± (pasta grafiÄŸi)
- Kategori daÄŸÄ±lÄ±mlarÄ± (bar grafiÄŸi)
- KÃ¼me-kategori iliÅŸki matrisi (heatmap)
- Her kÃ¼me iÃ§in kelime bulutlarÄ±
- Optimal k analizi grafikleri

### ğŸ“‹ Analiz Ã‡Ä±ktÄ±larÄ±
- DetaylÄ± kÃ¼me analizi raporu
- Her kÃ¼me iÃ§in anahtar kelimeler
- Kategori daÄŸÄ±lÄ±mlarÄ±
- Ã–rnek makale baÅŸlÄ±klarÄ±

## ğŸ”§ ArXiv Kategoriler

Projede varsayÄ±lan olarak ÅŸu ArXiv kategorileri kullanÄ±lÄ±r:

- `cs.AI` - Artificial Intelligence
- `cs.ML` - Machine Learning  
- `cs.CV` - Computer Vision
- `cs.CL` - Natural Language Processing
- `cs.LG` - Learning
- `stat.ML` - Machine Learning (Statistics)
- `physics.data-an` - Data Analysis
- `q-bio.QM` - Quantitative Methods
- `econ.EM` - Econometrics
- `math.ST` - Statistics Theory

## ğŸ“ Ã‡Ä±ktÄ± DosyalarÄ±

Proje Ã§alÄ±ÅŸtÄ±ktan sonra ÅŸu dosyalar oluÅŸur:

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ arxiv_papers.csv          # Ham ArXiv verileri
â”‚   â””â”€â”€ clustered_papers.csv      # KÃ¼melenmiÅŸ veriler
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ cluster_sizes.html        # Ä°nteraktif pasta grafiÄŸi  
â”‚   â”œâ”€â”€ category_distribution.png # Kategori daÄŸÄ±lÄ±mÄ±
â”‚   â”œâ”€â”€ cluster_category_heatmap.png # KÃ¼me-kategori iliÅŸkisi
â”‚   â”œâ”€â”€ cluster_wordclouds.png    # Kelime bulutlarÄ±
â”‚   â””â”€â”€ optimal_k_analysis.png    # Optimal k analizi
â””â”€â”€ akademik_makaleler_raporu.txt # DetaylÄ± rapor
```

## ğŸ¯ Ã–rnek KullanÄ±m SenaryolarÄ±

### 1. AraÅŸtÄ±rma AlanlarÄ± KeÅŸfetme
```bash
python main.py --full-pipeline --max-results 2000
```

### 2. Belirli Kategorilere Odaklanma
Kod iÃ§inde `categories` listesini deÄŸiÅŸtirerek istediÄŸiniz kategorileri seÃ§ebilirsiniz.

### 3. BÃ¼yÃ¼k Veri Analizi  
```bash
python main.py --full-pipeline --max-results 10000 --vocab-size 15000
```

## ğŸ› Sorun Giderme

### Virtual Environment Sorunu
```bash
# Virtual environment'Ä± yeniden oluÅŸturun
rm -rf academic_env
python3 -m venv academic_env
source academic_env/bin/activate
pip install -r requirements.txt
```

### Java HatasÄ±
```bash
# Java 8+ yÃ¼klÃ¼ olduÄŸundan emin olun
sudo apt install openjdk-11-jdk  # Ubuntu iÃ§in
```

### Memory HatasÄ±
```bash
# Spark memory ayarlarÄ± (bÃ¼yÃ¼k veri setleri iÃ§in)
export SPARK_DRIVER_MEMORY=4g
export SPARK_EXECUTOR_MEMORY=4g
```

### ArXiv API Rate Limiting
EÄŸer Ã§ok hÄ±zlÄ± istek gÃ¶nderiyorsanÄ±z, `arxiv_data_collector.py` dosyasÄ±nda `delay` parametresini artÄ±rÄ±n.

SorunlarÄ±nÄ±z iÃ§in aÅŸaÄŸÄ±daki kontrol listesini takip edin:

- [ ] Python 3.7+ yÃ¼klÃ¼ mÃ¼?
- [ ] Java 8+ yÃ¼klÃ¼ mÃ¼?
- [ ] Virtual environment aktif mi?
- [ ] requirements.txt yÃ¼klendi mi?
- [ ] Ä°nternet baÄŸlantÄ±sÄ± var mÄ±? (ArXiv API iÃ§in)# BigDataProject
